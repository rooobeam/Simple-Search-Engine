import re
import jieba

# text = ['再生医学',
# '美国对于贫铀炸弹的姿态',
# '“911”恐怖袭击后美国经济发生了什么变化',
# '各国对洛克比空难的反应',
# '关于科索沃战争中的暴行、军事干涉、独立申明以及其他任何与之有关的事情',
# '尼泊尔统治家族（王室）事变的背景和详情以及后继的来自各方的反应',
# '针对印尼华人的暴力事件',
# '美国政府起诉微软垄断行为，诉讼的内容、案例的事实、和解过程以及最后的判决',
# '各国已经进行的核武器试验',
# '叙利亚对中东和平进程的立场',
# '美国在线（AOL）和网景（Netscape）的关系',
# '什么是厄尔尼诺',
# '中俄之间发生的事情',
# '什么是温室气体',
# '北约和波兰的关系',
# '泰国在亚洲经济危机中的角色及其对世界经济的影响，各国针对亚洲经济危机所采取的步骤']
# 定义分词函数


def chinese_word_cut(text):
    jieba.initialize()  # 初始化jieba
    # 文本预处理 ：去除一些无用的字符只提取出中文出来
    new_data = re.findall('[\u4e00-\u9fa5]+', text, re.S)
    new_data = " ".join(new_data)
    # 文本分词
    seg_list_exact = jieba.lcut(new_data)
    result_list = []
    # 读取停用词库
    with open(r"D:\PyProject\停用词.txt", encoding='utf-8') as f:  # 可根据需要打开停用词库，然后加上不想显示的词语
        con = f.readlines()
        stop_words = set()
        for i in con:
            i = i.replace("\n", "")  # 去掉读取每一行数据的\n
            stop_words.add(i)
    # 去除停用词
    for word in seg_list_exact:
        if word not in stop_words:
            result_list.append(word)

    print(result_list)

    list = [['再生', '医学'],
    ['美国', '贫铀', '炸弹', '姿态'],
    ['恐怖袭击', '美国', '经济', '发生', '变化'],
    ['各国', '洛克比', '空难', '反应'],
    ['科索沃', '战争', '中', '暴行', ' ', '军事', '干涉', ' ', '独立', '申明', '事情'],
    ['尼泊尔', '统治', '家族', ' ', '王室', ' ', '事变', '背景', '详情', '后继', '各方', '反应'],
    ['印尼', '华人', '暴力事件'],
    ['美国政府', '起诉', '微软', '垄断', '行为', ' ', '诉讼', '内容', ' ', '案例', '事实', ' ', '和解', '过程', '最后', '判决'],
    ['各国', '已经', '进行', '核武器', '试验'],
    ['叙利亚', '中东', '和平', '进程', '立场'],
    ['美国在线', ' ', '网景', ' ', '关系'],
    ['厄尔尼诺'],
    ['中', '俄', '之间', '发生', '事情'],
    ['温室', '气体'],
    ['北约', '波兰', '关系'],
    ['泰国', '亚洲', '经济危机', '中', '角色', '世界', '经济', '影响', ' ', '各国', '亚洲', '经济危机', '采取', '步骤']]

    ind = 0
    for i in range(17):
        if result_list[0] == list[i][0]:
            ind = i
            break
    # print('ind', ind)
    return result_list, ind
